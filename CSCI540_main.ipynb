{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ee0f51a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk) (2022.4.24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "!pip install nltk\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# data_path = Path(\"/mnt/f/workspace/DataCLUE/nips_expr/aclImdb\")\n",
    "data_path = Path(\"/home/jovyan/Data Mining Grad Project/IMDB\")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "# stopwords = set(Path(\"/mnt/f/workspace/DataCLUE/nips_expr/stopwords.txt\").read_text().split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "afd3fdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 1368.07it/s]\n",
      "1000it [00:00, 1393.14it/s]\n",
      "1000it [00:00, 1370.13it/s]\n",
      "1000it [00:00, 1344.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def load_data(path):\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for file_path in tqdm((path / \"neg\").glob(\"*.txt\")):\n",
    "        labels.append(0)\n",
    "        texts.append(file_path.read_text())\n",
    "    \n",
    "    for file_path in tqdm((path / \"pos\").glob(\"*.txt\")):\n",
    "        labels.append(1)\n",
    "        texts.append(file_path.read_text())\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "all_train_texts, train_Y = load_data(data_path / \"train\")\n",
    "all_test_texts, test_Y = load_data(data_path / \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d519961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def data_quality(vectorizer, X, train_Y, cut_prop=1, filter_set = set()):\n",
    "    from sklearn.feature_selection import f_classif, chi2\n",
    "    F, pvalues_f = chi2(X, train_Y)\n",
    "    \n",
    "    last = int(len(F) * cut_prop)\n",
    "    sorted_F = sorted(zip(vectorizer.get_feature_names(), F), key=lambda x: x[1], reverse=True)[:last]\n",
    "    \n",
    "    values = [value for name, value in sorted_F if name not in filter_set]\n",
    "    # max_value, min_value = max(values), min(values)\n",
    "\n",
    "\n",
    "\n",
    "    # values = [(value - min_value) / (max_value - min_value) for value in values]\n",
    "\n",
    "    # print(values[:10])\n",
    "    # print(values[-10:])\n",
    "\n",
    "    return sum(values) / len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc0971d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_classifier(train_x, train_y, test_x):  \n",
    "    from sklearn.naive_bayes import MultinomialNB  \n",
    "    model = MultinomialNB(alpha=0.01)  \n",
    "    model.fit(train_x, train_y)\n",
    "    return model.predict(test_x)\n",
    "\n",
    "def svm_classifier(train_x, train_y, test_x):\n",
    "    from sklearn.svm import SVC  \n",
    "    model = SVC(kernel='rbf', probability=True, max_iter=50)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model.predict(test_x)\n",
    "\n",
    "def lgb_classifier(train_x, train_y, test_x):\n",
    "    !pip install lightgbm\n",
    "    import lightgbm as lgb\n",
    "    import numpy as np\n",
    "    \n",
    "    train_data = lgb.Dataset(train_x.astype('float32'), label=np.array(train_y, np.float32))\n",
    "#     test_data = lgb.Dataset(test_x, label=test_y)\n",
    "    \n",
    "    params={\n",
    "#         'learning_rate':0.1,\n",
    "#         'lambda_l1':0.1,\n",
    "#         'lambda_l2':0.2,\n",
    "#         'max_depth':6,\n",
    "#         'min_data_'\n",
    "        'objective':'multiclass',\n",
    "        'num_iteration': 200, \n",
    "        'num_class': 2,\n",
    "    }\n",
    "    \n",
    "    model = lgb.train(params, train_data)\n",
    "    return np.argmax(model.predict(test_x.astype('float32')), axis=1)\n",
    "\n",
    "def fasttext_classifier(train_texts, train_y, test_texts, test_y, name):\n",
    "    !pip install fasttext\n",
    "    import fasttext\n",
    "\n",
    "    model_path = data_path / f\"fasttext.model.{name}.bin\"\n",
    "    if model_path.exists():\n",
    "        classifier = fasttext.load_model(str(model_path))\n",
    "    else:\n",
    "        write_path = data_path / f\"fasttext.data.train.{name}.txt\"\n",
    "        write_path.write_text(\"\\n\".join(\n",
    "            f\"{sentence}\\t__label__{label}\" for sentence, label in zip(train_texts, train_Y)\n",
    "        ))\n",
    "\n",
    "        classifier = fasttext.train_supervised(str(write_path), lr=0.1, word_ngrams=3, bucket=2000000, label_prefix=\"__label__\")\n",
    "        classifier.save_model(str(model_path))\n",
    "        \n",
    "    labels, _ = classifier.predict(test_texts)\n",
    "    y_pred = [int(label[0][-1]) for label in labels]\n",
    "    return y_pred\n",
    "\n",
    "def knn_classifier(train_X, train_Y, test_X):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    model = KNeighborsClassifier(n_neighbors=3)  \n",
    "    model.fit(train_X, train_Y)\n",
    "    return model.predict(test_X)\n",
    "\n",
    "def textcnn(train_x, train_y):\n",
    "    from deepclassifier.models import TextCNN\n",
    "    from deepclassifier.trainers import Trainer\n",
    "    import torch.optim as optim\n",
    "    \n",
    "    model = TextCNN(embedding_dim=300, dropout_rate=0.2, num_class=2)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "772ee804",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mapper = {\n",
    "    \"nb\": naive_bayes_classifier,\n",
    "    \"svm\": svm_classifier,\n",
    "    \"lgb\": lgb_classifier,\n",
    "    \"knn\": knn_classifier,\n",
    "    \"fasttext\": None,\n",
    "    \"textcnn\": None,\n",
    "}\n",
    "\n",
    "def train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, model_name):\n",
    "    if model_name not in model_mapper:\n",
    "        y_pred = fasttext_classifier(train_texts, train_Y, test_texts, test_Y, model_name)\n",
    "    else:\n",
    "        y_pred = model_mapper[model_name](train_X, train_Y, test_X)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    return accuracy_score(test_Y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe4aa5e",
   "metadata": {},
   "source": [
    "# No Operations_Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c71d25ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase=False)\n",
    "train_X = vectorizer.fit_transform(all_train_texts)\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import f_classif, chi2\n",
    "F, pvalues_f = chi2(train_X, train_Y)\n",
    "\n",
    "sorted_F = sorted(zip(vectorizer.get_feature_names(), F), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a1a169f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and rent a GOOD horror movie. It\\'s like the writer had never seen a horror movie before and didn\\'t realize every single thing he wrote was clichéd and hackneyed and has been parodied to perfection in movies like \"Scream\" and \"Scary Movie\".<br /><br />In between the scary bits is the most BANAL and BORING dialog ever written. Stupid \"we\\'re going to the prom\" junk. I wanted to claw my ears off. Honestly, \"The Hills\" has better dialog.<br /><br />There really was no need to make this movie. Leading lady is uninteresting and I kept thinking \"Her? Really? Guy is obsessed with her? Really?\" <br /><br />All the characters act in stupid ways, including the police. (Cover the place in teams of 2! Front and back! Not one sleepy cop sitting in his car with the window rolled down just waiting for his throat to be slashed.) <br /><br />The serial killer just swans about murdering everyone he wants without the least bit of problem. No resistance from victims (or doors). Nobody has any protection or the least idea of fighting back (or flipping the security lock on the hotel room door). The people are like mentally disabled sheep.<br /><br />By the by, if you\\'re a gore fan, you\\'ll be disappointed too. All the killing is kept offscreen and is -- ahem -- tastefully done. (So boo hoo for you!) <br /><br />None of the killings is the least bit interesting. Most of the time they\\'ve already happened by the time we find out.<br /><br />The only cliché missing was the cat that always pops out in this kind of movies. \"Oh kitty! You scared me! I thought you were the killer -- AIIEEEE!\" <br /><br />And then at the end when it\\'s time for the killer to die -- well, let\\'s just say it\\'s the easiest and most obvious choice. Snore.<br /><br />The audience was jeering and talking back to the screen throughout. It was too dumb to believe and not really scary enough. Don\\'t encourage this kind of lazy film-making.<br /><br />(Oh, and by the way -- no crowning of a prom king or queen. No tiara. No bucket of blood.) <br /><br />So save your money and rent \"Carrie\" or \"Friday the 13th\" or \"Halloween\" or \"Scream\" or \"Scary Movie\" (any of them) to get a good scare with some original twists.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55b671d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9660682259796485"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=False)\n",
    "train_X = vectorizer.fit_transform(all_train_texts)\n",
    "test_X = vectorizer.transform(all_test_texts)\n",
    "\n",
    "data_quality(vectorizer, train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "685f889e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7385"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(all_train_texts, train_X, train_Y, all_test_texts, test_X, test_Y, \"nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe5e1617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.529"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(all_train_texts, train_X, train_Y, all_test_texts, test_X, test_Y, \"svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46ba5388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /opt/conda/lib/python3.9/site-packages (3.3.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.8.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.21.6)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7675\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1948\n",
      "[LightGBM] [Info] Start training from score -0.693147\n",
      "[LightGBM] [Info] Start training from score -0.693147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8055"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(all_train_texts, train_X, train_Y, all_test_texts, test_X, test_Y, \"lgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f9bb9bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pybind11>=2.2\n",
      "  Using cached pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from fasttext) (62.1.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from fasttext) (1.21.6)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp39-cp39-linux_x86_64.whl size=308257 sha256=4a6db45150eab5b19b8087f85995b2857afbf1beb5c482e4138fdb3b4dcb4491\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/64/57/bc/1741406019061d5664914b070bd3e71f6244648732bc96109e\n",
      "Successfully built fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "Successfully installed fasttext-0.9.2 pybind11-2.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  52283\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  187711 lr:  0.000000 avg.loss:  0.694590 ETA:   0h 0m 0s100.0% words/sec/thread:  187713 lr: -0.000034 avg.loss:  0.694590 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5175"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(all_train_texts, train_X, train_Y, all_test_texts, test_X, test_Y, \"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "690150e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5535"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(all_train_texts, train_X, train_Y, all_test_texts, test_X, test_Y, \"knn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad1712",
   "metadata": {},
   "source": [
    "# LowerCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d1526d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.095419358521762"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train_texts = [sentence.lower() for sentence in all_train_texts]\n",
    "test_texts = [sentence.lower() for sentence in all_test_texts]\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True)\n",
    "train_X = vectorizer.fit_transform(train_texts)\n",
    "test_X = vectorizer.transform(test_texts)\n",
    "\n",
    "data_quality(vectorizer, train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "23efa947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and rent a good horror movie. it\\'s like the writer had never seen a horror movie before and didn\\'t realize every single thing he wrote was clichéd and hackneyed and has been parodied to perfection in movies like \"scream\" and \"scary movie\".<br /><br />in between the scary bits is the most banal and boring dialog ever written. stupid \"we\\'re going to the prom\" junk. i wanted to claw my ears off. honestly, \"the hills\" has better dialog.<br /><br />there really was no need to make this movie. leading lady is uninteresting and i kept thinking \"her? really? guy is obsessed with her? really?\" <br /><br />all the characters act in stupid ways, including the police. (cover the place in teams of 2! front and back! not one sleepy cop sitting in his car with the window rolled down just waiting for his throat to be slashed.) <br /><br />the serial killer just swans about murdering everyone he wants without the least bit of problem. no resistance from victims (or doors). nobody has any protection or the least idea of fighting back (or flipping the security lock on the hotel room door). the people are like mentally disabled sheep.<br /><br />by the by, if you\\'re a gore fan, you\\'ll be disappointed too. all the killing is kept offscreen and is -- ahem -- tastefully done. (so boo hoo for you!) <br /><br />none of the killings is the least bit interesting. most of the time they\\'ve already happened by the time we find out.<br /><br />the only cliché missing was the cat that always pops out in this kind of movies. \"oh kitty! you scared me! i thought you were the killer -- aiieeee!\" <br /><br />and then at the end when it\\'s time for the killer to die -- well, let\\'s just say it\\'s the easiest and most obvious choice. snore.<br /><br />the audience was jeering and talking back to the screen throughout. it was too dumb to believe and not really scary enough. don\\'t encourage this kind of lazy film-making.<br /><br />(oh, and by the way -- no crowning of a prom king or queen. no tiara. no bucket of blood.) <br /><br />so save your money and rent \"carrie\" or \"friday the 13th\" or \"halloween\" or \"scream\" or \"scary movie\" (any of them) to get a good scare with some original twists.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03f71775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7355"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7e610e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5175"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4a32c79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /opt/conda/lib/python3.9/site-packages (3.3.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.8.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.21.6)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102661 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7658\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1904\n",
      "[LightGBM] [Info] Start training from score -0.693147\n",
      "[LightGBM] [Info] Start training from score -0.693147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8115"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"lgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f16e94ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /opt/conda/lib/python3.9/site-packages (0.9.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.9/site-packages (from fasttext) (2.9.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from fasttext) (1.21.6)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from fasttext) (62.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  48062\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  197380 lr:  0.000000 avg.loss:  0.695870 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6395"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"lower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "88e0c434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5605"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"knn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d925c7",
   "metadata": {},
   "source": [
    "# Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ecff5f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 11/2000 [00:00<00:19, 102.03it/s]\u001b[A\n",
      "  1%|▏         | 28/2000 [00:00<00:14, 139.83it/s]\u001b[A\n",
      "  2%|▏         | 44/2000 [00:00<00:13, 145.43it/s]\u001b[A\n",
      "  3%|▎         | 59/2000 [00:00<00:14, 138.52it/s]\u001b[A\n",
      "  4%|▎         | 74/2000 [00:00<00:13, 141.73it/s]\u001b[A\n",
      "  4%|▍         | 89/2000 [00:00<00:14, 127.51it/s]\u001b[A\n",
      "  5%|▌         | 103/2000 [00:00<00:15, 122.50it/s]\u001b[A\n",
      "  6%|▌         | 118/2000 [00:00<00:14, 125.63it/s]\u001b[A\n",
      "  7%|▋         | 135/2000 [00:01<00:13, 136.87it/s]\u001b[A\n",
      "  7%|▋         | 149/2000 [00:01<00:13, 135.75it/s]\u001b[A\n",
      "  9%|▊         | 173/2000 [00:01<00:11, 160.34it/s]\u001b[A\n",
      " 10%|▉         | 190/2000 [00:01<00:11, 162.27it/s]\u001b[A\n",
      " 10%|█         | 207/2000 [00:01<00:11, 162.98it/s]\u001b[A\n",
      " 11%|█▏        | 229/2000 [00:01<00:10, 176.98it/s]\u001b[A\n",
      " 12%|█▏        | 247/2000 [00:01<00:10, 161.50it/s]\u001b[A\n",
      " 13%|█▎        | 264/2000 [00:01<00:10, 160.58it/s]\u001b[A\n",
      " 14%|█▍        | 281/2000 [00:01<00:11, 150.12it/s]\u001b[A\n",
      " 15%|█▌        | 300/2000 [00:02<00:10, 159.84it/s]\u001b[A\n",
      " 16%|█▌        | 320/2000 [00:02<00:09, 170.48it/s]\u001b[A\n",
      " 17%|█▋        | 338/2000 [00:02<00:10, 161.02it/s]\u001b[A\n",
      " 18%|█▊        | 355/2000 [00:02<00:10, 156.94it/s]\u001b[A\n",
      " 19%|█▊        | 371/2000 [00:02<00:10, 155.68it/s]\u001b[A\n",
      " 19%|█▉        | 387/2000 [00:02<00:10, 149.18it/s]\u001b[A\n",
      " 20%|██        | 403/2000 [00:02<00:10, 148.97it/s]\u001b[A\n",
      " 21%|██        | 421/2000 [00:02<00:10, 155.86it/s]\u001b[A\n",
      " 22%|██▏       | 437/2000 [00:02<00:10, 155.22it/s]\u001b[A\n",
      " 23%|██▎       | 453/2000 [00:03<00:10, 149.23it/s]\u001b[A\n",
      " 23%|██▎       | 469/2000 [00:03<00:10, 146.01it/s]\u001b[A\n",
      " 24%|██▍       | 484/2000 [00:03<00:11, 136.02it/s]\u001b[A\n",
      " 25%|██▌       | 501/2000 [00:03<00:10, 144.63it/s]\u001b[A\n",
      " 26%|██▌       | 518/2000 [00:03<00:09, 150.80it/s]\u001b[A\n",
      " 27%|██▋       | 534/2000 [00:03<00:09, 147.91it/s]\u001b[A\n",
      " 28%|██▊       | 552/2000 [00:03<00:09, 154.14it/s]\u001b[A\n",
      " 28%|██▊       | 568/2000 [00:03<00:09, 150.46it/s]\u001b[A\n",
      " 29%|██▉       | 589/2000 [00:03<00:08, 166.94it/s]\u001b[A\n",
      " 30%|███       | 606/2000 [00:04<00:08, 157.22it/s]\u001b[A\n",
      " 31%|███       | 622/2000 [00:04<00:09, 138.59it/s]\u001b[A\n",
      " 32%|███▏      | 637/2000 [00:04<00:09, 141.50it/s]\u001b[A\n",
      " 33%|███▎      | 653/2000 [00:04<00:09, 144.82it/s]\u001b[A\n",
      " 33%|███▎      | 668/2000 [00:04<00:09, 136.70it/s]\u001b[A\n",
      " 34%|███▍      | 687/2000 [00:04<00:08, 150.92it/s]\u001b[A\n",
      " 35%|███▌      | 705/2000 [00:04<00:08, 157.74it/s]\u001b[A\n",
      " 36%|███▌      | 722/2000 [00:04<00:08, 147.61it/s]\u001b[A\n",
      " 37%|███▋      | 738/2000 [00:05<00:09, 126.39it/s]\u001b[A\n",
      " 38%|███▊      | 754/2000 [00:05<00:09, 134.27it/s]\u001b[A\n",
      " 38%|███▊      | 769/2000 [00:05<00:09, 135.43it/s]\u001b[A\n",
      " 39%|███▉      | 784/2000 [00:05<00:09, 134.25it/s]\u001b[A\n",
      " 40%|███▉      | 798/2000 [00:05<00:09, 133.10it/s]\u001b[A\n",
      " 41%|████      | 812/2000 [00:05<00:10, 117.12it/s]\u001b[A\n",
      " 41%|████▏     | 827/2000 [00:05<00:09, 124.86it/s]\u001b[A\n",
      " 42%|████▏     | 844/2000 [00:05<00:08, 136.66it/s]\u001b[A\n",
      " 43%|████▎     | 862/2000 [00:05<00:07, 146.68it/s]\u001b[A\n",
      " 44%|████▍     | 878/2000 [00:06<00:08, 137.73it/s]\u001b[A\n",
      " 45%|████▍     | 897/2000 [00:06<00:07, 150.65it/s]\u001b[A\n",
      " 46%|████▌     | 913/2000 [00:06<00:08, 135.82it/s]\u001b[A\n",
      " 46%|████▋     | 928/2000 [00:06<00:08, 124.47it/s]\u001b[A\n",
      " 47%|████▋     | 943/2000 [00:06<00:08, 130.19it/s]\u001b[A\n",
      " 48%|████▊     | 957/2000 [00:06<00:07, 131.86it/s]\u001b[A\n",
      " 49%|████▉     | 975/2000 [00:06<00:07, 144.21it/s]\u001b[A\n",
      " 50%|████▉     | 990/2000 [00:06<00:07, 142.51it/s]\u001b[A\n",
      " 50%|█████     | 1005/2000 [00:06<00:07, 141.68it/s]\u001b[A\n",
      " 51%|█████     | 1020/2000 [00:07<00:07, 137.82it/s]\u001b[A\n",
      " 52%|█████▏    | 1034/2000 [00:07<00:07, 131.19it/s]\u001b[A\n",
      " 52%|█████▏    | 1048/2000 [00:07<00:07, 120.17it/s]\u001b[A\n",
      " 53%|█████▎    | 1061/2000 [00:07<00:07, 122.24it/s]\u001b[A\n",
      " 54%|█████▍    | 1076/2000 [00:07<00:07, 128.96it/s]\u001b[A\n",
      " 55%|█████▍    | 1090/2000 [00:07<00:07, 119.52it/s]\u001b[A\n",
      " 55%|█████▌    | 1103/2000 [00:07<00:07, 119.82it/s]\u001b[A\n",
      " 56%|█████▌    | 1119/2000 [00:07<00:07, 125.14it/s]\u001b[A\n",
      " 57%|█████▋    | 1132/2000 [00:08<00:07, 121.94it/s]\u001b[A\n",
      " 57%|█████▋    | 1145/2000 [00:08<00:07, 115.05it/s]\u001b[A\n",
      " 58%|█████▊    | 1158/2000 [00:08<00:07, 110.60it/s]\u001b[A\n",
      " 59%|█████▊    | 1173/2000 [00:08<00:06, 120.39it/s]\u001b[A\n",
      " 60%|█████▉    | 1192/2000 [00:08<00:05, 135.14it/s]\u001b[A\n",
      " 60%|██████    | 1206/2000 [00:08<00:06, 131.31it/s]\u001b[A\n",
      " 61%|██████    | 1220/2000 [00:08<00:05, 133.66it/s]\u001b[A\n",
      " 62%|██████▏   | 1240/2000 [00:08<00:05, 148.06it/s]\u001b[A\n",
      " 63%|██████▎   | 1255/2000 [00:08<00:05, 128.33it/s]\u001b[A\n",
      " 64%|██████▍   | 1276/2000 [00:09<00:04, 147.06it/s]\u001b[A\n",
      " 65%|██████▍   | 1292/2000 [00:09<00:04, 142.82it/s]\u001b[A\n",
      " 65%|██████▌   | 1307/2000 [00:09<00:04, 144.70it/s]\u001b[A\n",
      " 66%|██████▌   | 1322/2000 [00:09<00:04, 142.04it/s]\u001b[A\n",
      " 67%|██████▋   | 1337/2000 [00:09<00:04, 136.87it/s]\u001b[A\n",
      " 68%|██████▊   | 1351/2000 [00:09<00:04, 137.70it/s]\u001b[A\n",
      " 68%|██████▊   | 1368/2000 [00:09<00:04, 146.03it/s]\u001b[A\n",
      " 69%|██████▉   | 1383/2000 [00:09<00:04, 143.13it/s]\u001b[A\n",
      " 70%|███████   | 1401/2000 [00:09<00:03, 153.20it/s]\u001b[A\n",
      " 71%|███████   | 1421/2000 [00:10<00:03, 160.51it/s]\u001b[A\n",
      " 72%|███████▏  | 1441/2000 [00:10<00:03, 170.95it/s]\u001b[A\n",
      " 73%|███████▎  | 1459/2000 [00:10<00:03, 154.69it/s]\u001b[A\n",
      " 74%|███████▍  | 1475/2000 [00:10<00:03, 155.63it/s]\u001b[A\n",
      " 75%|███████▍  | 1494/2000 [00:10<00:03, 164.28it/s]\u001b[A\n",
      " 76%|███████▌  | 1511/2000 [00:10<00:03, 162.44it/s]\u001b[A\n",
      " 76%|███████▋  | 1528/2000 [00:10<00:02, 158.13it/s]\u001b[A\n",
      " 77%|███████▋  | 1546/2000 [00:10<00:02, 163.95it/s]\u001b[A\n",
      " 78%|███████▊  | 1563/2000 [00:10<00:02, 151.62it/s]\u001b[A\n",
      " 79%|███████▉  | 1581/2000 [00:11<00:02, 159.27it/s]\u001b[A\n",
      " 80%|███████▉  | 1598/2000 [00:11<00:02, 155.80it/s]\u001b[A\n",
      " 81%|████████  | 1614/2000 [00:11<00:02, 142.07it/s]\u001b[A\n",
      " 82%|████████▏ | 1631/2000 [00:11<00:02, 146.24it/s]\u001b[A\n",
      " 82%|████████▏ | 1649/2000 [00:11<00:02, 153.47it/s]\u001b[A\n",
      " 83%|████████▎ | 1666/2000 [00:11<00:02, 157.51it/s]\u001b[A\n",
      " 84%|████████▍ | 1682/2000 [00:11<00:02, 154.27it/s]\u001b[A\n",
      " 85%|████████▍ | 1698/2000 [00:11<00:01, 155.74it/s]\u001b[A\n",
      " 86%|████████▌ | 1717/2000 [00:11<00:01, 160.70it/s]\u001b[A\n",
      " 87%|████████▋ | 1734/2000 [00:12<00:01, 145.04it/s]\u001b[A\n",
      " 87%|████████▋ | 1749/2000 [00:12<00:01, 133.88it/s]\u001b[A\n",
      " 88%|████████▊ | 1765/2000 [00:12<00:01, 140.07it/s]\u001b[A\n",
      " 89%|████████▉ | 1780/2000 [00:12<00:01, 133.98it/s]\u001b[A\n",
      " 90%|████████▉ | 1794/2000 [00:12<00:01, 133.10it/s]\u001b[A\n",
      " 90%|█████████ | 1808/2000 [00:12<00:01, 134.72it/s]\u001b[A\n",
      " 91%|█████████▏| 1826/2000 [00:12<00:01, 146.93it/s]\u001b[A\n",
      " 92%|█████████▏| 1844/2000 [00:12<00:01, 153.83it/s]\u001b[A\n",
      " 93%|█████████▎| 1865/2000 [00:12<00:00, 168.03it/s]\u001b[A\n",
      " 94%|█████████▍| 1882/2000 [00:13<00:00, 157.25it/s]\u001b[A\n",
      " 95%|█████████▍| 1898/2000 [00:13<00:00, 157.95it/s]\u001b[A\n",
      " 96%|█████████▌| 1914/2000 [00:13<00:00, 157.46it/s]\u001b[A\n",
      " 96%|█████████▋| 1930/2000 [00:13<00:00, 153.73it/s]\u001b[A\n",
      " 98%|█████████▊| 1950/2000 [00:13<00:00, 165.73it/s]\u001b[A\n",
      " 98%|█████████▊| 1967/2000 [00:13<00:00, 159.67it/s]\u001b[A\n",
      "100%|██████████| 2000/2000 [00:13<00:00, 144.81it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 20/2000 [00:00<00:10, 197.01it/s]\u001b[A\n",
      "  2%|▏         | 40/2000 [00:00<00:14, 139.07it/s]\u001b[A\n",
      "  3%|▎         | 56/2000 [00:00<00:14, 136.06it/s]\u001b[A\n",
      "  4%|▎         | 71/2000 [00:00<00:14, 134.57it/s]\u001b[A\n",
      "  4%|▍         | 85/2000 [00:00<00:14, 133.96it/s]\u001b[A\n",
      "  5%|▍         | 99/2000 [00:00<00:14, 132.26it/s]\u001b[A\n",
      "  6%|▌         | 115/2000 [00:00<00:13, 140.18it/s]\u001b[A\n",
      "  6%|▋         | 130/2000 [00:00<00:13, 139.74it/s]\u001b[A\n",
      "  7%|▋         | 145/2000 [00:01<00:14, 132.43it/s]\u001b[A\n",
      "  8%|▊         | 160/2000 [00:01<00:13, 135.25it/s]\u001b[A\n",
      "  9%|▉         | 180/2000 [00:01<00:11, 153.29it/s]\u001b[A\n",
      " 10%|▉         | 196/2000 [00:01<00:12, 142.87it/s]\u001b[A\n",
      " 11%|█         | 211/2000 [00:01<00:12, 138.04it/s]\u001b[A\n",
      " 12%|█▏        | 231/2000 [00:01<00:12, 145.36it/s]\u001b[A\n",
      " 12%|█▏        | 247/2000 [00:01<00:11, 149.16it/s]\u001b[A\n",
      " 13%|█▎        | 263/2000 [00:01<00:11, 147.71it/s]\u001b[A\n",
      " 14%|█▍        | 278/2000 [00:01<00:12, 140.89it/s]\u001b[A\n",
      " 15%|█▍        | 293/2000 [00:02<00:12, 133.07it/s]\u001b[A\n",
      " 15%|█▌        | 307/2000 [00:02<00:12, 132.55it/s]\u001b[A\n",
      " 16%|█▋        | 326/2000 [00:02<00:11, 141.89it/s]\u001b[A\n",
      " 17%|█▋        | 344/2000 [00:02<00:11, 150.00it/s]\u001b[A\n",
      " 18%|█▊        | 361/2000 [00:02<00:10, 152.68it/s]\u001b[A\n",
      " 19%|█▉        | 380/2000 [00:02<00:10, 161.30it/s]\u001b[A\n",
      " 20%|█▉        | 397/2000 [00:02<00:10, 158.99it/s]\u001b[A\n",
      " 21%|██        | 413/2000 [00:02<00:10, 155.82it/s]\u001b[A\n",
      " 22%|██▏       | 432/2000 [00:02<00:09, 163.49it/s]\u001b[A\n",
      " 23%|██▎       | 451/2000 [00:03<00:09, 169.79it/s]\u001b[A\n",
      " 23%|██▎       | 469/2000 [00:03<00:10, 149.25it/s]\u001b[A\n",
      " 24%|██▍       | 486/2000 [00:03<00:09, 153.68it/s]\u001b[A\n",
      " 25%|██▌       | 502/2000 [00:03<00:09, 152.34it/s]\u001b[A\n",
      " 26%|██▌       | 518/2000 [00:03<00:10, 139.85it/s]\u001b[A\n",
      " 27%|██▋       | 534/2000 [00:03<00:10, 140.86it/s]\u001b[A\n",
      " 27%|██▋       | 549/2000 [00:03<00:10, 142.05it/s]\u001b[A\n",
      " 28%|██▊       | 564/2000 [00:03<00:10, 136.40it/s]\u001b[A\n",
      " 29%|██▉       | 579/2000 [00:04<00:10, 138.20it/s]\u001b[A\n",
      " 30%|██▉       | 593/2000 [00:04<00:10, 133.40it/s]\u001b[A\n",
      " 30%|███       | 610/2000 [00:04<00:09, 143.28it/s]\u001b[A\n",
      " 31%|███▏      | 625/2000 [00:04<00:09, 139.22it/s]\u001b[A\n",
      " 32%|███▏      | 643/2000 [00:04<00:09, 147.65it/s]\u001b[A\n",
      " 33%|███▎      | 659/2000 [00:04<00:08, 149.31it/s]\u001b[A\n",
      " 34%|███▍      | 676/2000 [00:04<00:08, 153.14it/s]\u001b[A\n",
      " 35%|███▍      | 692/2000 [00:04<00:09, 142.65it/s]\u001b[A\n",
      " 35%|███▌      | 707/2000 [00:04<00:09, 135.00it/s]\u001b[A\n",
      " 36%|███▋      | 726/2000 [00:05<00:08, 147.89it/s]\u001b[A\n",
      " 37%|███▋      | 742/2000 [00:05<00:08, 141.55it/s]\u001b[A\n",
      " 38%|███▊      | 760/2000 [00:05<00:08, 149.56it/s]\u001b[A\n",
      " 39%|███▉      | 776/2000 [00:05<00:08, 145.49it/s]\u001b[A\n",
      " 40%|███▉      | 791/2000 [00:05<00:08, 138.74it/s]\u001b[A\n",
      " 40%|████      | 810/2000 [00:05<00:07, 152.02it/s]\u001b[A\n",
      " 41%|████▏     | 826/2000 [00:05<00:07, 152.44it/s]\u001b[A\n",
      " 42%|████▏     | 842/2000 [00:05<00:07, 153.42it/s]\u001b[A\n",
      " 43%|████▎     | 858/2000 [00:05<00:08, 138.60it/s]\u001b[A\n",
      " 44%|████▎     | 873/2000 [00:06<00:07, 141.50it/s]\u001b[A\n",
      " 45%|████▍     | 891/2000 [00:06<00:07, 151.69it/s]\u001b[A\n",
      " 45%|████▌     | 907/2000 [00:06<00:07, 147.58it/s]\u001b[A\n",
      " 46%|████▌     | 922/2000 [00:06<00:08, 133.82it/s]\u001b[A\n",
      " 47%|████▋     | 936/2000 [00:06<00:08, 131.46it/s]\u001b[A\n",
      " 48%|████▊     | 954/2000 [00:06<00:07, 142.58it/s]\u001b[A\n",
      " 49%|████▊     | 974/2000 [00:06<00:06, 155.32it/s]\u001b[A\n",
      " 50%|████▉     | 991/2000 [00:06<00:06, 159.37it/s]\u001b[A\n",
      " 50%|█████     | 1008/2000 [00:06<00:06, 155.79it/s]\u001b[A\n",
      " 51%|█████     | 1024/2000 [00:07<00:06, 151.81it/s]\u001b[A\n",
      " 52%|█████▏    | 1040/2000 [00:07<00:06, 144.77it/s]\u001b[A\n",
      " 53%|█████▎    | 1055/2000 [00:07<00:06, 140.19it/s]\u001b[A\n",
      " 54%|█████▎    | 1074/2000 [00:07<00:06, 153.12it/s]\u001b[A\n",
      " 55%|█████▍    | 1091/2000 [00:07<00:05, 155.33it/s]\u001b[A\n",
      " 56%|█████▌    | 1112/2000 [00:07<00:05, 169.81it/s]\u001b[A\n",
      " 56%|█████▋    | 1130/2000 [00:07<00:05, 166.28it/s]\u001b[A\n",
      " 57%|█████▊    | 1150/2000 [00:07<00:05, 166.32it/s]\u001b[A\n",
      " 58%|█████▊    | 1167/2000 [00:07<00:05, 158.36it/s]\u001b[A\n",
      " 59%|█████▉    | 1184/2000 [00:08<00:05, 160.50it/s]\u001b[A\n",
      " 60%|██████    | 1201/2000 [00:08<00:05, 154.53it/s]\u001b[A\n",
      " 61%|██████    | 1220/2000 [00:08<00:04, 164.14it/s]\u001b[A\n",
      " 62%|██████▏   | 1237/2000 [00:08<00:05, 146.12it/s]\u001b[A\n",
      " 63%|██████▎   | 1257/2000 [00:08<00:04, 159.47it/s]\u001b[A\n",
      " 64%|██████▎   | 1274/2000 [00:08<00:05, 144.20it/s]\u001b[A\n",
      " 65%|██████▍   | 1292/2000 [00:08<00:04, 151.83it/s]\u001b[A\n",
      " 65%|██████▌   | 1308/2000 [00:08<00:04, 152.07it/s]\u001b[A\n",
      " 66%|██████▌   | 1324/2000 [00:08<00:04, 146.01it/s]\u001b[A\n",
      " 67%|██████▋   | 1343/2000 [00:09<00:04, 157.76it/s]\u001b[A\n",
      " 68%|██████▊   | 1360/2000 [00:09<00:04, 157.16it/s]\u001b[A\n",
      " 69%|██████▉   | 1376/2000 [00:09<00:04, 155.70it/s]\u001b[A\n",
      " 70%|██████▉   | 1392/2000 [00:09<00:03, 152.10it/s]\u001b[A\n",
      " 70%|███████   | 1410/2000 [00:09<00:03, 158.14it/s]\u001b[A\n",
      " 71%|███████▏  | 1426/2000 [00:09<00:04, 132.94it/s]\u001b[A\n",
      " 72%|███████▏  | 1447/2000 [00:09<00:03, 152.32it/s]\u001b[A\n",
      " 73%|███████▎  | 1464/2000 [00:09<00:03, 153.15it/s]\u001b[A\n",
      " 74%|███████▍  | 1480/2000 [00:10<00:03, 142.95it/s]\u001b[A\n",
      " 75%|███████▍  | 1495/2000 [00:10<00:03, 135.93it/s]\u001b[A\n",
      " 75%|███████▌  | 1509/2000 [00:10<00:03, 135.40it/s]\u001b[A\n",
      " 76%|███████▌  | 1523/2000 [00:10<00:03, 133.45it/s]\u001b[A\n",
      " 77%|███████▋  | 1542/2000 [00:10<00:03, 143.87it/s]\u001b[A\n",
      " 78%|███████▊  | 1558/2000 [00:10<00:03, 139.24it/s]\u001b[A\n",
      " 79%|███████▊  | 1573/2000 [00:10<00:03, 136.17it/s]\u001b[A\n",
      " 79%|███████▉  | 1587/2000 [00:10<00:03, 134.79it/s]\u001b[A\n",
      " 80%|████████  | 1601/2000 [00:10<00:03, 126.76it/s]\u001b[A\n",
      " 81%|████████  | 1619/2000 [00:11<00:02, 140.16it/s]\u001b[A\n",
      " 82%|████████▏ | 1638/2000 [00:11<00:02, 152.94it/s]\u001b[A\n",
      " 83%|████████▎ | 1655/2000 [00:11<00:02, 157.48it/s]\u001b[A\n",
      " 84%|████████▎ | 1671/2000 [00:11<00:02, 147.61it/s]\u001b[A\n",
      " 84%|████████▍ | 1687/2000 [00:11<00:02, 150.58it/s]\u001b[A\n",
      " 85%|████████▌ | 1706/2000 [00:11<00:01, 158.69it/s]\u001b[A\n",
      " 86%|████████▌ | 1723/2000 [00:11<00:01, 148.96it/s]\u001b[A\n",
      " 87%|████████▋ | 1741/2000 [00:11<00:01, 155.73it/s]\u001b[A\n",
      " 88%|████████▊ | 1757/2000 [00:11<00:01, 137.97it/s]\u001b[A\n",
      " 89%|████████▊ | 1773/2000 [00:12<00:01, 139.52it/s]\u001b[A\n",
      " 89%|████████▉ | 1789/2000 [00:12<00:01, 144.10it/s]\u001b[A\n",
      " 90%|█████████ | 1804/2000 [00:12<00:01, 134.20it/s]\u001b[A\n",
      " 91%|█████████ | 1818/2000 [00:12<00:01, 129.65it/s]\u001b[A\n",
      " 92%|█████████▏| 1837/2000 [00:12<00:01, 145.21it/s]\u001b[A\n",
      " 93%|█████████▎| 1852/2000 [00:12<00:01, 146.15it/s]\u001b[A\n",
      " 93%|█████████▎| 1867/2000 [00:12<00:00, 145.51it/s]\u001b[A\n",
      " 94%|█████████▍| 1884/2000 [00:12<00:00, 150.33it/s]\u001b[A\n",
      " 95%|█████████▌| 1900/2000 [00:12<00:00, 144.14it/s]\u001b[A\n",
      " 96%|█████████▌| 1916/2000 [00:13<00:00, 146.80it/s]\u001b[A\n",
      " 97%|█████████▋| 1931/2000 [00:13<00:00, 141.78it/s]\u001b[A\n",
      " 97%|█████████▋| 1946/2000 [00:13<00:00, 141.31it/s]\u001b[A\n",
      " 98%|█████████▊| 1961/2000 [00:13<00:00, 140.67it/s]\u001b[A\n",
      " 99%|█████████▉| 1980/2000 [00:13<00:00, 150.60it/s]\u001b[A\n",
      "100%|██████████| 2000/2000 [00:13<00:00, 146.08it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def stem_sentences(sentences: str) -> str:\n",
    "    def stem_sentence(sentence):\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        return \" \".join(\n",
    "            porter.stem(word, to_lowercase=False) for word in words\n",
    "        )\n",
    "    return [stem_sentence(sentence) for sentence in tqdm(sentences)]\n",
    "\n",
    "train_texts, test_texts = stem_sentences(all_train_texts), stem_sentences(all_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c0ae3f1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3851499590692637"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True)\n",
    "train_X = vectorizer.fit_transform(train_texts)\n",
    "test_X = vectorizer.transform(test_texts)\n",
    "\n",
    "data_quality(vectorizer, train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dc7fa80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7285"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "faa9ffcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5125"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3db40450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /opt/conda/lib/python3.9/site-packages (3.3.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.0.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.8.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.21.6)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7649\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1824\n",
      "[LightGBM] [Info] Start training from score -0.693147\n",
      "[LightGBM] [Info] Start training from score -0.693147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8065"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"lgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5283db5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /opt/conda/lib/python3.9/site-packages (0.9.2)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from fasttext) (62.1.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from fasttext) (1.21.6)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.9/site-packages (from fasttext) (2.9.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  23721\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  203069 lr:  0.000000 avg.loss:  0.694611 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5005"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"stem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ce88e387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5695"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"knn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb421f",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c29a8016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d4d4e4f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/2000 [03:23<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  0%|          | 0/2000 [00:25<?, ?it/s]\n",
      "  0%|          | 0/2000 [00:12<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 1/2000 [00:01<57:08,  1.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|▏         | 29/2000 [00:01<01:29, 21.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 58/2000 [00:01<00:40, 48.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 86/2000 [00:02<00:25, 76.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 111/2000 [00:02<00:18, 101.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 140/2000 [00:02<00:13, 134.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 179/2000 [00:02<00:09, 185.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 11%|█         | 216/2000 [00:02<00:07, 226.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▏        | 249/2000 [00:02<00:07, 244.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 14%|█▍        | 281/2000 [00:02<00:06, 254.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|█▌        | 317/2000 [00:02<00:05, 280.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 349/2000 [00:02<00:05, 283.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|█▉        | 381/2000 [00:02<00:05, 272.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|██        | 411/2000 [00:03<00:05, 276.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 444/2000 [00:03<00:05, 290.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|██▍       | 475/2000 [00:03<00:05, 275.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 504/2000 [00:03<00:05, 278.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|██▋       | 534/2000 [00:03<00:05, 284.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|██▊       | 563/2000 [00:03<00:05, 284.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|██▉       | 599/2000 [00:03<00:04, 300.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 32%|███▏      | 630/2000 [00:03<00:05, 262.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 658/2000 [00:04<00:05, 261.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 34%|███▍      | 686/2000 [00:04<00:04, 263.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|███▌      | 716/2000 [00:04<00:04, 273.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|███▋      | 744/2000 [00:04<00:05, 246.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 770/2000 [00:04<00:05, 243.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|███▉      | 795/2000 [00:04<00:05, 229.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|████      | 819/2000 [00:04<00:05, 219.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 42%|████▏     | 848/2000 [00:04<00:04, 237.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 44%|████▍     | 876/2000 [00:04<00:04, 246.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▌     | 903/2000 [00:05<00:04, 251.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 46%|████▋     | 929/2000 [00:05<00:04, 230.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|████▊     | 961/2000 [00:05<00:04, 253.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|████▉     | 991/2000 [00:05<00:03, 265.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 51%|█████     | 1021/2000 [00:05<00:03, 275.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 1049/2000 [00:05<00:03, 263.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████▍    | 1077/2000 [00:05<00:03, 267.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▌    | 1105/2000 [00:05<00:03, 254.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████▋    | 1131/2000 [00:05<00:03, 247.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████▊    | 1157/2000 [00:06<00:03, 243.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▉    | 1188/2000 [00:06<00:03, 260.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|██████    | 1216/2000 [00:06<00:02, 265.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▏   | 1243/2000 [00:06<00:02, 265.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|██████▎   | 1270/2000 [00:06<00:02, 262.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████▍   | 1297/2000 [00:06<00:02, 263.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████▋   | 1326/2000 [00:06<00:02, 270.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████▊   | 1354/2000 [00:06<00:02, 269.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████▉   | 1385/2000 [00:06<00:02, 280.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████   | 1424/2000 [00:06<00:01, 312.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████▎  | 1456/2000 [00:07<00:01, 302.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▍  | 1492/2000 [00:07<00:01, 315.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████▌  | 1524/2000 [00:07<00:01, 315.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 1559/2000 [00:07<00:01, 319.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|███████▉  | 1591/2000 [00:07<00:01, 317.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████  | 1623/2000 [00:07<00:01, 306.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████▎ | 1654/2000 [00:07<00:01, 294.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████▍ | 1684/2000 [00:07<00:01, 287.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████▌ | 1715/2000 [00:07<00:00, 293.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████▋ | 1745/2000 [00:08<00:00, 269.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 89%|████████▊ | 1773/2000 [00:08<00:00, 271.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████ | 1801/2000 [00:08<00:00, 260.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████▏| 1834/2000 [00:08<00:00, 279.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|█████████▎| 1873/2000 [00:08<00:00, 304.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▌| 1904/2000 [00:08<00:00, 295.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████▋| 1934/2000 [00:08<00:00, 296.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████▊| 1964/2000 [00:08<00:00, 290.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 2000/2000 [00:08<00:00, 224.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████| 2000/2000 [00:06<00:00, 303.32it/s]\n"
     ]
    }
   ],
   "source": [
    "def lemma_sentences(sentences: str) -> str:\n",
    "    def lemma_sentence(sentence):\n",
    "        return \" \".join(\n",
    "            wordnet_lemmatizer.lemmatize(word) for word in nltk.word_tokenize(sentence)\n",
    "        )\n",
    "    return [lemma_sentence(sentence) for sentence in tqdm(sentences)]\n",
    "\n",
    "train_texts, test_texts = lemma_sentences(all_train_texts), lemma_sentences(all_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d76841b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.162227993528516"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True)\n",
    "train_X = vectorizer.fit_transform(train_texts)\n",
    "test_X = vectorizer.transform(test_texts)\n",
    "\n",
    "data_quality(vectorizer, train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8e20e203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c9db572a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5285"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2b18d0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /opt/conda/lib/python3.9/site-packages (3.3.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.21.6)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.0.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.8.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7585\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1861\n",
      "[LightGBM] [Info] Start training from score -0.693147\n",
      "[LightGBM] [Info] Start training from score -0.693147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.812"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"lgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ca17e360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /opt/conda/lib/python3.9/site-packages (0.9.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from fasttext) (1.21.6)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from fasttext) (62.1.0)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.9/site-packages (from fasttext) (2.9.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  29056\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  207516 lr:  0.000000 avg.loss:  0.695225 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.508"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"lemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d6a25f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5595"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"knn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611f5f7d",
   "metadata": {},
   "source": [
    "# Remove Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9937aac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 156.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"rent GOOD horror movie . It 's like writer never seen horror movie n't realize every single thing wrote clichéd hackneyed parodied perfection movies like `` Scream '' `` Scary Movie '' . < br / > < br / > In scary bits BANAL BORING dialog ever written . Stupid `` 're going prom '' junk . I wanted claw ears . Honestly , `` The Hills '' better dialog. < br / > < br / > There really need make movie . Leading lady uninteresting I kept thinking `` Her ? Really ? Guy obsessed ? Really ? '' < br / > < br / > All characters act stupid ways , including police . ( Cover place teams 2 ! Front back ! Not one sleepy cop sitting car window rolled waiting throat slashed . ) < br / > < br / > The serial killer swans murdering everyone wants without least bit problem . No resistance victims ( doors ) . Nobody protection least idea fighting back ( flipping security lock hotel room door ) . The people like mentally disabled sheep. < br / > < br / > By , 're gore fan , 'll disappointed . All killing kept offscreen -- ahem -- tastefully done . ( So boo hoo ! ) < br / > < br / > None killings least bit interesting . Most time 've already happened time find out. < br / > < br / > The cliché missing cat always pops kind movies . `` Oh kitty ! You scared ! I thought killer -- AIIEEEE ! '' < br / > < br / > And end 's time killer die -- well , let 's say 's easiest obvious choice . Snore. < br / > < br / > The audience jeering talking back screen throughout . It dumb believe really scary enough . Do n't encourage kind lazy film-making. < br / > < br / > ( Oh , way -- crowning prom king queen . No tiara . No bucket blood . ) < br / > < br / > So save money rent `` Carrie '' `` Friday 13th '' `` Halloween '' `` Scream '' `` Scary Movie '' ( ) get good scare original twists .\"]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stop_sentences([all_train_texts[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6ac54ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:05<00:00, 388.11it/s]\n",
      "100%|██████████| 2000/2000 [00:05<00:00, 397.28it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_stop_sentences(sentences: str) -> str:\n",
    "    def remove_stop_sentence(sentence):\n",
    "        return \" \".join(\n",
    "            word for word in nltk.word_tokenize(sentence) if word not in stopWords\n",
    "        )\n",
    "    return [remove_stop_sentence(sentence) for sentence in tqdm(sentences)]\n",
    "\n",
    "train_texts, test_texts = remove_stop_sentences(all_train_texts), remove_stop_sentences(all_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "136ea990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.04981558911457"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True)\n",
    "train_X = vectorizer.fit_transform(train_texts)\n",
    "test_X = vectorizer.transform(test_texts)\n",
    "\n",
    "data_quality(vectorizer, train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dbcf1d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.726"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "eb196a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5075"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5e6930b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /opt/conda/lib/python3.9/site-packages (3.3.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.8.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.21.6)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.181268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6858\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1860\n",
      "[LightGBM] [Info] Start training from score -0.693147\n",
      "[LightGBM] [Info] Start training from score -0.693147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8135"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"lgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7238882b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /opt/conda/lib/python3.9/site-packages (0.9.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from fasttext) (1.21.6)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from fasttext) (62.1.0)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.9/site-packages (from fasttext) (2.9.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  31113\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  205149 lr:  0.000000 avg.loss:  0.694947 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5475"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"rem_stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1c9b6c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5375"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"knn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde3447",
   "metadata": {},
   "source": [
    "# f_classify threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0d15e359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:04<00:00, 468.34it/s]\n",
      "100%|██████████| 2000/2000 [00:04<00:00, 465.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.3417035155926635"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=False)\n",
    "train_X = vectorizer.fit_transform(all_train_texts)\n",
    "test_X = vectorizer.transform(all_test_texts)\n",
    "\n",
    "from sklearn.feature_selection import f_classif\n",
    "F, pvalues_f = f_classif(train_X, train_Y)\n",
    "value_names = sorted(zip(vectorizer.get_feature_names(), F), key=lambda x: x[1], reverse=True)\n",
    "filter_names = [name for name, value in value_names[-int(len(value_names)*0.2):]]\n",
    "\n",
    "def remove_words_sentences(sentences: str, removed_words: set) -> str:\n",
    "    def remove_words_sentence(sentence):\n",
    "        return \" \".join(\n",
    "            word for word in nltk.word_tokenize(sentence) if word not in removed_words\n",
    "        )\n",
    "    return [remove_words_sentence(sentence) for sentence in tqdm(sentences)]\n",
    "\n",
    "train_texts, test_texts = remove_words_sentences(all_train_texts, set(filter_names)), remove_words_sentences(all_test_texts, set(filter_names))\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=False)\n",
    "train_X = vectorizer.fit_transform(train_texts)\n",
    "test_X = vectorizer.transform(test_texts)\n",
    "\n",
    "data_quality(vectorizer, train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "113227f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7305"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ba88a336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5225"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a7da77a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /opt/conda/lib/python3.9/site-packages (3.3.2)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.0.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.21.6)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.8.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.103318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5690\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1419\n",
      "[LightGBM] [Info] Start training from score -0.693147\n",
      "[LightGBM] [Info] Start training from score -0.693147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"lgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "38d48fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /opt/conda/lib/python3.9/site-packages (0.9.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.9/site-packages (from fasttext) (2.9.2)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from fasttext) (62.1.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from fasttext) (1.21.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  25894\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  221908 lr:  0.000000 avg.loss:  0.694734 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"remove_f_thre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "08ca1592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.554"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"knn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a9b233",
   "metadata": {},
   "source": [
    "# CHI2_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7eaa4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase=False)\n",
    "train_X = vectorizer.fit_transform(all_train_texts)\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "F, pvalues_f = chi2(train_X, train_Y)\n",
    "value_names = sorted(zip(vectorizer.get_feature_names(), F), key=lambda x: x[1], reverse=True)\n",
    "filter_names = [name for name, value in value_names[-int(len(value_names)*0.4):]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "989e2c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 56.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"and rent a GOOD horror movie . 's like the had a horror movie and n't realize single thing he wrote was clichéd and hackneyed and has been to perfection in like `` '' and `` Scary Movie '' . < br / > < br / > between the scary bits is the most BANAL and BORING dialog ever . Stupid `` 're going to the prom '' junk . I to claw my off . Honestly , `` Hills '' has better dialog. < br / > < br / > There really was no to make this movie . is uninteresting and I thinking `` ? Really ? Guy is obsessed with her ? Really ? '' < br / > < br / > the act in stupid ways , the police . ( Cover the in of 2 ! and ! Not one sleepy cop sitting in his with the just for his throat to be slashed . ) < br / > < br / > serial killer just about he without the least bit of problem . No from victims ( or doors ) . Nobody has any or the least idea of ( or the security on the hotel ) . like mentally sheep. < br / > < br / > the , if 're a gore fan , 'll be disappointed too . the killing is and is -- -- . ( So boo for ! ) < br / > < br / > of the killings is the least bit interesting . Most of the they 've the find out. < br / > < br / > only cliché was the that always out in this kind of . `` Oh ! You scared ! I thought were the killer -- AIIEEEE ! '' < br / > < br / > then at the end when it 's for the killer to die -- well , let 's just say it 's the and most obvious choice . Snore. < br / > < br / > audience was and talking to the throughout . was too dumb to believe and not really scary enough . Do n't encourage this kind of lazy film-making. < br / > < br / > ( Oh , and the -- no of a prom king or queen . No . No of blood . ) < br / > < br / > So save your money and rent `` '' or `` Friday the 13th '' or `` Halloween '' or `` '' or `` Scary Movie '' ( any of ) to get a scare with some original twists .\"]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_words_sentences([all_train_texts[0]], set(filter_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "49c2c055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:04<00:00, 458.62it/s]\n",
      "100%|██████████| 2000/2000 [00:04<00:00, 467.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.719204270388087"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts, test_texts = remove_words_sentences(all_train_texts, set(filter_names)), remove_words_sentences(all_test_texts, set(filter_names))\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=False)\n",
    "train_X = vectorizer.fit_transform(train_texts)\n",
    "test_X = vectorizer.transform(test_texts)\n",
    "\n",
    "data_quality(vectorizer, train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1583b38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3f058549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3d953e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5265"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f7af6970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /opt/conda/lib/python3.9/site-packages (3.3.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.21.6)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.8.0)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104744 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5328\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1302\n",
      "[LightGBM] [Info] Start training from score -0.693147\n",
      "[LightGBM] [Info] Start training from score -0.693147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.804"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"lgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9814a0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /opt/conda/lib/python3.9/site-packages (0.9.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.9/site-packages (from fasttext) (2.9.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from fasttext) (1.21.6)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from fasttext) (62.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  20772\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  220663 lr:  0.000000 avg.loss:  0.695797 ETA:   0h 0m 0s100.0% words/sec/thread:  220665 lr: -0.000020 avg.loss:  0.695797 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5665"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"chi2_04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2ecc9ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.555"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"knn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae37bf1",
   "metadata": {},
   "source": [
    "# CHI2_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d545864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "54662ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4142181209029294"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=False)\n",
    "train_X = vectorizer.fit_transform(all_train_texts)\n",
    "test_X = vectorizer.transform(all_test_texts)\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "F, pvalues_f = chi2(train_X, train_Y)\n",
    "value_names = sorted(zip(vectorizer.get_feature_names(), F), key=lambda x: x[1], reverse=True)\n",
    "filter_names = [name for name, value in value_names[-int(len(value_names)*0.2):]]\n",
    "\n",
    "data_quality(vectorizer, train_X, train_Y, 1, set(filter_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "32c8f600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:04<00:00, 466.88it/s]\n",
      "100%|██████████| 2000/2000 [00:04<00:00, 464.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.3431034490585363"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts, test_texts = remove_words_sentences(all_train_texts, set(filter_names)), remove_words_sentences(all_test_texts, set(filter_names))\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=False)\n",
    "train_X = vectorizer.fit_transform(train_texts)\n",
    "test_X = vectorizer.transform(test_texts)\n",
    "data_quality(vectorizer, train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4d0cc27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.732"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3475628b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.524"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "69800f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /opt/conda/lib/python3.9/site-packages (3.3.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.21.6)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.0.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.8.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100478 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5712\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1414\n",
      "[LightGBM] [Info] Start training from score -0.693147\n",
      "[LightGBM] [Info] Start training from score -0.693147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8055"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"lgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9121bf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /opt/conda/lib/python3.9/site-packages (0.9.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from fasttext) (1.21.6)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.9/site-packages (from fasttext) (2.9.2)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from fasttext) (62.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  25893\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  218403 lr:  0.000000 avg.loss:  0.693890 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.501"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"chi2_02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "08a269f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5625"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_texts, train_X, train_Y, test_texts, test_X, test_Y, \"knn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0f8f5bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)  \n",
    "neigh.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f06fd364",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = neigh.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6035b-728a-4d60-973b-6a88e29e4a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efcb157-1aa0-4f24-ac57-139ea497269b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c63b7b314f0f8feaa4096db9046dfe18edc0516c1ae347727067c69fc71ca872"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
